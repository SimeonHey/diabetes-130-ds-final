{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "class DatasetHolder:\n",
    "    def __init__(self, X_train, y_train, X_dev, y_dev, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.X_dev = X_dev\n",
    "        self.y_dev = y_dev\n",
    "        \n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "def split_simple(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return DatasetHolder(X_train, y_train, X_dev, y_dev, X_test, y_test)\n",
    "\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_dataset(path):\n",
    "    print(f\"Loading dataset from {path}...\")\n",
    "    df = load_csv(path)\n",
    "    X_full, y_full = df.drop('next_time_in_hospital', axis=1).values, df['next_time_in_hospital'].values\n",
    "    print(f\"Shape of dataset: {X_full.shape}\")\n",
    "    \n",
    "    return split_simple(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from nn_allcats_ds.csv...\n",
      "Shape of dataset: (30248, 1903)\n"
     ]
    }
   ],
   "source": [
    "simple = load_dataset(\"nn_allcats_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "import time\n",
    "\n",
    "def train_model_for_seconds(dataset, model, seconds=30, batch_size=None):\n",
    "    start_time = time.time()\n",
    "    # your code\n",
    "    \n",
    "    epochs = 0\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    \n",
    "    while time.time() - start_time < seconds:\n",
    "        cur = model.fit(dataset.X_train, dataset.y_train, batch_size=batch_size, epochs=1, \n",
    "                        validation_data=(dataset.X_dev, dataset.y_dev))\n",
    "        \n",
    "        for el in cur.history['loss']:\n",
    "            loss_hist.append(el)\n",
    "        \n",
    "        for el in cur.history['val_loss']:\n",
    "            val_loss_hist.append(el)\n",
    "        \n",
    "        epochs += 1\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Trained model on {epochs} epochs for time {elapsed} secs ({epochs/elapsed} epochs in second)\")\n",
    "    \n",
    "    print(\"Loss history:\")\n",
    "    plt.plot(range(len(loss_hist)), loss_hist, label='loss')\n",
    "    plt.plot(range(len(val_loss_hist)), val_loss_hist, label='val_loss')\n",
    "\n",
    "def eval_model(dataset, model, batch_size=None):\n",
    "    train_model_for_seconds(dataset, model, batch_size=batch_size)\n",
    "    predictions = model.predict(dataset.X_dev)\n",
    "    ret = f\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}\"# , poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\n",
    "    \n",
    "    diffs = dataset.y_dev - predictions[:, 0]\n",
    "    plt.hist(diffs)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_perceptron_m(dataset):\n",
    "    perceptron_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(dataset.X_train.shape[1],))\n",
    "    ])\n",
    "    perceptron_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return perceptron_m\n",
    "\n",
    "perceptron_m = get_perceptron_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_two_layer_m(dataset):\n",
    "    two_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    two_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return two_layer_m\n",
    "\n",
    "two_layer_m = get_two_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_layer_m(dataset):\n",
    "    three_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    three_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return three_layer_m\n",
    "\n",
    "three_layer_m = get_three_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_four_layer_m(dataset):\n",
    "    four_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    four_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return four_layer_m\n",
    "\n",
    "four_layer_m = get_four_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_expdec_four_layer_m(dataset):\n",
    "    expdec_four_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/2), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/4), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    expdec_four_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return expdec_four_layer_m\n",
    "\n",
    "expdec_four_layer_m = get_expdec_four_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expdec_three_layer_m(dataset):\n",
    "    expdec_three_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/2), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    expdec_three_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return expdec_three_layer_m\n",
    "\n",
    "expdec_three_layer_m = get_expdec_three_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expdec_smallstart_three_layer_m(dataset):\n",
    "    expdec_smallstart_three_layer_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/4), input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/8), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    expdec_smallstart_three_layer_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    return expdec_smallstart_three_layer_m\n",
    "\n",
    "expdec_smallstart_three_layer_m = get_expdec_smallstart_three_layer_m(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 2.9436 - val_loss: 2.9427\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 2.8266 - val_loss: 2.8881\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 2.7174 - val_loss: 2.8362\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 31us/sample - loss: 2.6153 - val_loss: 2.7870\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 199us/sample - loss: 2.5199 - val_loss: 2.7404\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 113us/sample - loss: 2.4306 - val_loss: 2.6960\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 10us/sample - loss: 2.3470 - val_loss: 2.6538\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 2.2686 - val_loss: 2.6137\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 2.1951 - val_loss: 2.5755\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 39us/sample - loss: 2.1261 - val_loss: 2.5391\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 88us/sample\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[19358,1903] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Cast (defined at <ipython-input-137-ff4dbddebb5a>:15) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_297129]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-21bee4567dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_perceptron_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-137-ff4dbddebb5a>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}\"\u001b[0m\u001b[0;31m# , poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-ff4dbddebb5a>\u001b[0m in \u001b[0;36mtrain_model_for_seconds\u001b[0;34m(dataset, model, seconds, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         cur = model.fit(dataset.X_train, dataset.y_train, batch_size=batch_size, epochs=1, \n\u001b[0;32m---> 15\u001b[0;31m                         validation_data=(dataset.X_dev, dataset.y_dev))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[19358,1903] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Cast (defined at <ipython-input-137-ff4dbddebb5a>:15) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_297129]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "eval_model(simple, get_perceptron_m(simple), batch_size=simple.X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 17s 862us/sample - loss: 8.5468 - val_loss: 1.4097\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 17s 864us/sample - loss: 2.0686 - val_loss: 6.3922\n",
      "Trained model on 2 epochs for time 33.88205361366272 secs (0.059028299252602355 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-d5bf8a0ce9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo_layer_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-b973c2764d1e>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}, poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_dev\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_tweedie_deviance\u001b[0;34m(y_true, y_pred, sample_weight, power)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# Poisson distribution, y_true >= 0, y_pred > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             raise ValueError(message + \"non-negative y_true and strictly \"\n\u001b[0m\u001b[1;32m    737\u001b[0m                              \"positive y_pred.\")\n\u001b[1;32m    738\u001b[0m         \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxlogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(simple, two_layer_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 5s 234us/sample - loss: 1.1040\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 4s 225us/sample - loss: 973.0371\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 4s 223us/sample - loss: 40.1512\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 5s 240us/sample - loss: 2.7862\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 7s 348us/sample - loss: 5.6102\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 6s 331us/sample - loss: 2.5273\n",
      "Trained model on 6 epochs for time 31.084725379943848 secs (0.19302084630515204 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0304242383632964"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(simple, four_layer_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 73us/sample - loss: 1.7560\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 64us/sample - loss: 3.7428\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 1.2118\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 1.6500\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 1.2785\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 65us/sample - loss: 0.9768\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 64us/sample - loss: 0.9262\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9342\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 65us/sample - loss: 0.9285\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 64us/sample - loss: 0.9111\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8700\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8528\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8376\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8239\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 65us/sample - loss: 0.8110\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 74us/sample - loss: 0.7984\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 65us/sample - loss: 0.7861\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 71us/sample - loss: 0.7736\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 2s 83us/sample - loss: 0.7613\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 67us/sample - loss: 0.7491\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7373\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7258\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7150\n",
      "Trained model on 24 epochs for time 30.901013612747192 secs (0.7766735519025043 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0436421808098189"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(simple, expdec_four_layer_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 72us/sample - loss: 1.9829\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 64us/sample - loss: 16.2460\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 2.5041\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 2.1665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 4.0057\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 3.0615\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 1.7978\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 1.1043\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 64us/sample - loss: 0.8868\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9307\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9614\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9690\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9588\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9380\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.9122\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8851\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8587\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8351\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.8147\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7972\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7701\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7595\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 1s 63us/sample - loss: 0.7502\n",
      "Trained model on 25 epochs for time 31.064942836761475 secs (0.8047656849513216 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0472048344340843"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(simple, expdec_three_layer_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 2.5203\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 2.3404\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 2.1774\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 2.0306\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.8983\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.7797\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.6731\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.5780\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.4934\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 1.4186\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.3525\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.2946\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.2439\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.1996\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.1606\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.1263\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.0962\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.0695\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.0460\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.0253\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 1.0067\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9752\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9621\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9502\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9395\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9297\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9209\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9127\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.9052\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8984\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8922\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.8809\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8759\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8712\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8625\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8586\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.8548\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8513\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8480\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8448\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8419\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8390\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8362\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8335\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8310\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8285\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8261\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8239\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8217\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8196\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8177\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8158\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8139\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8121\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8104\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8087\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8070\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8054\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8038\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8023\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.8007\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7993\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7979\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7965\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7952\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7938\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7925\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7913\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7888\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7876\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7865\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7854\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7843\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7833\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7823\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7813\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7803\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7794\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7776\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7767\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7758\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7749\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7740\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7732\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7723\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7715\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7708\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7700\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7693\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7685\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7650\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7643\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7636\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7629\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7622\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 0.7616\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7609\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7603\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 0.7596\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 0.7590\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7583\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7577\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7570\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7564\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7557\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7551\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7544\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7538\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7531\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.7525\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.7519\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 17us/sample - loss: 0.7513\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 0.7507\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7500\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7494\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7488\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7483\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7477\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7471\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7465\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7460\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.7454\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7449\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 10us/sample - loss: 0.7443\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 12us/sample - loss: 0.7437\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 13us/sample - loss: 0.7432\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7426\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 0.7420\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 0.7415\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 9us/sample - loss: 0.7410\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7404\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7399\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7394\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7389\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7385\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.7380\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 11us/sample - loss: 0.7375\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 11us/sample - loss: 0.7370\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 13us/sample - loss: 0.7366\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 11us/sample - loss: 0.7361\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 10us/sample - loss: 0.7356\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 11us/sample - loss: 0.7352\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 0.7347\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 13us/sample - loss: 0.7342\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7337\n",
      "Trained model on 154 epochs for time 30.21278214454651 secs (5.097180367674198 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.110585223783406"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(simple, expdec_smallstart_three_layer_m) # Big overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried straightforward testing on this dataset, including\n",
    "* A single perceptron\n",
    "* Densely connected 2 full-width layers\n",
    "* Densely connected 4 full-width layers (overfits)\n",
    "* Densely connected 4 layers with exponentially decreasing witdth\n",
    "* Densely connected 3 layers -=-\n",
    "* Densely connected 3 lyaers with exponentially decreasing width, but starting from smaller width in general\n",
    "\n",
    "Those were only tried with ReLu activation function.\n",
    "\n",
    "Learning rates turned out to be very important, so I switched from SGD to Adam optimizer, which uses an adaptive learning rate.\n",
    "\n",
    "Turns out that models with smaller numer of units perform better (but too little become prone to overfitting)\n",
    "\n",
    "A thing to try would be to better engineer features, so we have smaller number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from nn_compdiags_ds.csv...\n",
      "Shape of dataset: (30248, 245)\n"
     ]
    }
   ],
   "source": [
    "compdiags = load_dataset(\"nn_compdiags_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 3.0292 - val_loss: 3.0285\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 2.9109 - val_loss: 2.9317\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.8008 - val_loss: 2.8415\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.6982 - val_loss: 2.7574\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.6024 - val_loss: 2.6787\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.5130 - val_loss: 2.6051\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.4294 - val_loss: 2.5362\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.3511 - val_loss: 2.4716\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.2777 - val_loss: 2.4109\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.2088 - val_loss: 2.3539\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.1441 - val_loss: 2.3003\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.0834 - val_loss: 2.2498\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 2.0262 - val_loss: 2.2022\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.9723 - val_loss: 2.1573\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.9216 - val_loss: 2.1148\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.8737 - val_loss: 2.0747\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.8286 - val_loss: 2.0368\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.7859 - val_loss: 2.0009\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.7456 - val_loss: 1.9669\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.7074 - val_loss: 1.9347\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.6713 - val_loss: 1.9041\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.6371 - val_loss: 1.8750\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.6047 - val_loss: 1.8474\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.5739 - val_loss: 1.8212\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.5448 - val_loss: 1.7963\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.5171 - val_loss: 1.7726\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.4908 - val_loss: 1.7500\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.4659 - val_loss: 1.7285\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.4421 - val_loss: 1.7080\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.4196 - val_loss: 1.6885\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3981 - val_loss: 1.6699\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3777 - val_loss: 1.6521\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3582 - val_loss: 1.6351\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3397 - val_loss: 1.6190\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3221 - val_loss: 1.6035\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.3052 - val_loss: 1.5887\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2892 - val_loss: 1.5746\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2739 - val_loss: 1.5611\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2593 - val_loss: 1.5482\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2454 - val_loss: 1.5359\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2321 - val_loss: 1.5240\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2195 - val_loss: 1.5127\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2073 - val_loss: 1.5019\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1958 - val_loss: 1.4915\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1847 - val_loss: 1.4816\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1742 - val_loss: 1.4721\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1641 - val_loss: 1.4630\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1544 - val_loss: 1.4543\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1452 - val_loss: 1.4459\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1364 - val_loss: 1.4379\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1279 - val_loss: 1.4302\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1198 - val_loss: 1.4228\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1121 - val_loss: 1.4157\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1047 - val_loss: 1.4089\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0976 - val_loss: 1.4024\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0908 - val_loss: 1.3961\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0843 - val_loss: 1.3901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0781 - val_loss: 1.3844\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0721 - val_loss: 1.3788\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0664 - val_loss: 1.3735\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0609 - val_loss: 1.3684\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0557 - val_loss: 1.3635\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0507 - val_loss: 1.3588\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0458 - val_loss: 1.3542\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0412 - val_loss: 1.3499\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0368 - val_loss: 1.3457\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0325 - val_loss: 1.3417\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0284 - val_loss: 1.3378\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0245 - val_loss: 1.3341\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0208 - val_loss: 1.3305\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0172 - val_loss: 1.3271\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0137 - val_loss: 1.3237\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0104 - val_loss: 1.3206\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0072 - val_loss: 1.3175\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0041 - val_loss: 1.3145\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0012 - val_loss: 1.3117\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 4us/sample - loss: 0.9983 - val_loss: 1.3090\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9956 - val_loss: 1.3063\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9930 - val_loss: 1.3038\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9905 - val_loss: 1.3014\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9881 - val_loss: 1.2990\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9858 - val_loss: 1.2968\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9836 - val_loss: 1.2946\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9814 - val_loss: 1.2925\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9794 - val_loss: 1.2905\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9774 - val_loss: 1.2885\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9755 - val_loss: 1.2866\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9737 - val_loss: 1.2848\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9719 - val_loss: 1.2831\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9702 - val_loss: 1.2814\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9686 - val_loss: 1.2798\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9670 - val_loss: 1.2782\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9655 - val_loss: 1.2767\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9640 - val_loss: 1.2753\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9626 - val_loss: 1.2739\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9613 - val_loss: 1.2725\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9600 - val_loss: 1.2712\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9587 - val_loss: 1.2700\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9575 - val_loss: 1.2688\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9564 - val_loss: 1.2676\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9553 - val_loss: 1.2665\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9542 - val_loss: 1.2654\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9532 - val_loss: 1.2643\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9522 - val_loss: 1.2633\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9512 - val_loss: 1.2623\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9503 - val_loss: 1.2614\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9494 - val_loss: 1.2605\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9485 - val_loss: 1.2596\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9477 - val_loss: 1.2587\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9469 - val_loss: 1.2579\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9461 - val_loss: 1.2571\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9454 - val_loss: 1.2564\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9446 - val_loss: 1.2556\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9439 - val_loss: 1.2549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9433 - val_loss: 1.2542\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9426 - val_loss: 1.2536\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9420 - val_loss: 1.2529\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9414 - val_loss: 1.2523\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9408 - val_loss: 1.2517\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9403 - val_loss: 1.2511\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9397 - val_loss: 1.2505\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9392 - val_loss: 1.2500\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9387 - val_loss: 1.2495\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9382 - val_loss: 1.2490\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9377 - val_loss: 1.2485\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9373 - val_loss: 1.2480\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9368 - val_loss: 1.2475\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9364 - val_loss: 1.2471\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9360 - val_loss: 1.2467\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9356 - val_loss: 1.2462\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9352 - val_loss: 1.2458\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9348 - val_loss: 1.2454\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9345 - val_loss: 1.2451\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9341 - val_loss: 1.2447\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9338 - val_loss: 1.2444\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9335 - val_loss: 1.2440\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9331 - val_loss: 1.2437\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.9328 - val_loss: 1.2434\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9325 - val_loss: 1.2430\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9322 - val_loss: 1.2427\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9320 - val_loss: 1.2425\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9317 - val_loss: 1.2422\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.9314 - val_loss: 1.2419\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9312 - val_loss: 1.2416\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9309 - val_loss: 1.2414\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9307 - val_loss: 1.2411\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9305 - val_loss: 1.2409\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9303 - val_loss: 1.2407\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9300 - val_loss: 1.2404\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9298 - val_loss: 1.2402\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 13us/sample - loss: 0.9296 - val_loss: 1.2400\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 5us/sample - loss: 0.9294 - val_loss: 1.2398\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9292 - val_loss: 1.2396\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9291 - val_loss: 1.2394\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9289 - val_loss: 1.2392\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9287 - val_loss: 1.2390\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 6us/sample - loss: 0.9285 - val_loss: 1.2389\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9284 - val_loss: 1.2387\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9282 - val_loss: 1.2385\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9281 - val_loss: 1.2384\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 0.9279 - val_loss: 1.2382\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9278 - val_loss: 1.2380\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9276 - val_loss: 1.2379\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9275 - val_loss: 1.2378\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9273 - val_loss: 1.2376\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9272 - val_loss: 1.2375\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 6us/sample - loss: 0.9271 - val_loss: 1.2374\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9270 - val_loss: 1.2372\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9268 - val_loss: 1.2371\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9267 - val_loss: 1.2370\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 146us/sample - loss: 0.9266 - val_loss: 1.2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 8us/sample - loss: 0.9265 - val_loss: 1.2368\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9264 - val_loss: 1.2366\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9263 - val_loss: 1.2365\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 182us/sample - loss: 0.9262 - val_loss: 1.2364\n",
      "Trained model on 175 epochs for time 30.788166046142578 secs (5.684002085012972 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error: 1.2364397530423024'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(compdiags, get_perceptron_m(compdiags), batch_size=compdiags.X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 26us/sample - loss: 1.1474\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 1.1125\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.9913\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 1.0369\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.9873\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.9318\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 16us/sample - loss: 0.9299\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 0.9217\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 0.8869\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8634\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8604\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8519\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8302\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8129\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.8063\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7983\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7830\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7693\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7627\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7567\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7462\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7358\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.7292\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7226\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7132\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.7044\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6990\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6937\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6858\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6782\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6730\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6611\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.6545\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6495\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6443\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6383\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6330\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6284\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6231\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6175\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6127\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.6081\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.6031\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5983\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5938\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5891\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5843\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5799\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5756\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5710\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5624\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5580\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5537\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5496\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5454\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5412\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5371\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5331\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5290\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5250\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5210\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.5170\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5131\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5092\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5053\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.5015\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4976\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4938\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4862\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4788\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4750\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4713\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4640\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4604\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.4567\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4531\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4459\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4424\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4388\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4353\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4318\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4283\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4248\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4214\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4179\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4145\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4111\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4077\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4043\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.4009\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.3975\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 14us/sample - loss: 0.3942\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 15us/sample - loss: 0.3909\n",
      "Trained model on 99 epochs for time 30.23174023628235 secs (3.274703977549597 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1449172894842226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(compdiags, get_two_layer_m(compdiags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 4s 188us/sample - loss: 1.0471\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 176us/sample - loss: 213.2748\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 11.7584\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 1.0086\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 1.6539\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 176us/sample - loss: 1.3225\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 1.0126\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 178us/sample - loss: 1.0594\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 4s 197us/sample - loss: 1.1375\n",
      "Trained model on 9 epochs for time 31.62170171737671 secs (0.28461466370275496 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0781475534342504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(compdiags, get_four_layer_m(compdiags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 202us/sample - loss: 0.9792 - val_loss: 0.9777\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      " 6592/19358 [=========>....................] - ETA: 2s - loss: 0.9362"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d3345531c410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompdiags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_expdec_four_layer_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompdiags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}\"\u001b[0m\u001b[0;31m# , poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36mtrain_model_for_seconds\u001b[0;34m(dataset, model, seconds, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         cur = model.fit(dataset.X_train, dataset.y_train, batch_size=batch_size, epochs=1, \n\u001b[0;32m---> 15\u001b[0;31m                         validation_data=(dataset.X_dev, dataset.y_dev))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_model(compdiags, get_expdec_four_layer_m(compdiags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 187us/sample - loss: 0.9822 - val_loss: 0.9723\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 175us/sample - loss: 0.9293 - val_loss: 0.9679\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 0.9076 - val_loss: 0.9625\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 174us/sample - loss: 0.8881 - val_loss: 0.9752\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 0.8647 - val_loss: 0.9957\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 171us/sample - loss: 0.8385 - val_loss: 0.9777\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 175us/sample - loss: 0.7956 - val_loss: 1.0561\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 175us/sample - loss: 0.7534 - val_loss: 1.0263\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 179us/sample - loss: 0.7107 - val_loss: 1.1073\n",
      "Trained model on 9 epochs for time 31.33046841621399 secs (0.28726030777574857 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error: 1.1072548098696722'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(compdiags, get_expdec_three_layer_m(compdiags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 11us/sample - loss: 1.2855\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.2657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.2469\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.2292\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.2125\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1967\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1819\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1681\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1551\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1430\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1316\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.1211\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1113\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.1023\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0938\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.0861\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0789\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.0722\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0603\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0549\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0500\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.0455\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0412\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0372\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0335\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.0300\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 1.0267\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0237\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 1.0208\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 1.0181\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0155\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0131\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0109\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0087\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0067\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 1.0048\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 1.0030\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 1.0013\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9997\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9982\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9968\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9954\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9940\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9927\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9915\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9903\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9892\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9882\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9871\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9861\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9852\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9842\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9834\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9816\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9808\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9800\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9792\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9784\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9777\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9769\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9762\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9755\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9748\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9741\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9734\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9728\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9721\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9715\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9709\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9703\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9697\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9691\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9685\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9673\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9652\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9646\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9635\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9630\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9625\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9620\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9615\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9610\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9605\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9600\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9595\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9590\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9585\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9580\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9576\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9571\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9566\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 4us/sample - loss: 0.9561\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9557\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9552\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9547\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9543\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 0.9538\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9533\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9529\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9524\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9520\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9515\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9511\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9506\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9502\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9497\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9493\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9489\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9485\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9480\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9476\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9472\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9468\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9464\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9460\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9456\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9453\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9449\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9445\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9441\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9437\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9433\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9430\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9426\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9422\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9418\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9415\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9411\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9407\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9404\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9400\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9397\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9393\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9390\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9387\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9384\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9381\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9377\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9374\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9371\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9368\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9365\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9362\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9359\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9356\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9353\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9350\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9347\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9343\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9340\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9337\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9334\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9331\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9327\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9324\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9321\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9319\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9316\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9310\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9307\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9304\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9302\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9299\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9296\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9293\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9290\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9287\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9284\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9281\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9279\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9276\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9274\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9271\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9269\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9266\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9264\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9261\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9259\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9256\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9254\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9252\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9249\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9247\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9245\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9242\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9240\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9238\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9236\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9233\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9231\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9229\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9227\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9225\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9223\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9220\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9218\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9216\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9213\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9211\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9209\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9207\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9204\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9202\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9200\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9197\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9195\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9193\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9191\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9188\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9186\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9184\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9181\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9179\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9177\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9175\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9173\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9171\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9168\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9166\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9164\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9162\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9160\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9158\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9156\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9155\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9153\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9151\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9149\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9147\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9146\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9144\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9142\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9141\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9139\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9138\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9136\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9135\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9133\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9132\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9130\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9127\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9126\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9125\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9123\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9122\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9120\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9119\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9117\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9116\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9114\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9113\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9112\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9110\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9109\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9108\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9106\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9105\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9104\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9102\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9101\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9099\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9098\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9097\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9095\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9094\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9092\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9091\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9089\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9088\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9087\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9085\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9084\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9082\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9081\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9079\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9078\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9076\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9075\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9073\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9072\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9070\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9069\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9068\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9066\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9065\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9063\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9062\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9061\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9060\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9058\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9057\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9056\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9055\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9054\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9052\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9051\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9050\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9049\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9048\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9047\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9046\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9044\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9043\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9042\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9041\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9040\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9039\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9038\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9037\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.9036\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9034\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9033\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9032\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9031\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9030\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9029\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9027\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9026\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9025\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9024\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9023\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9021\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9019\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9018\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9017\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9015\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9014\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9013\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9012\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9011\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9010\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9008\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9007\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9006\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9005\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9004\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9003\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9002\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9001\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.9000\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8999\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8998\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8997\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8996\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8995\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8994\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8993\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8992\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8991\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8990\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8989\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8988\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8987\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8986\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8985\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8984\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8983\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8982\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8981\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8980\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8979\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8978\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8977\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8976\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8975\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8974\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8973\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8972\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8971\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8970\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8970\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8969\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8968\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8967\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8966\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8965\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8964\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8963\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8962\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8962\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8961\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8960\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8959\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8958\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8957\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8957\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8956\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8955\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8955\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8954\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8953\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8952\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8952\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8951\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8950\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8949\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8949\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8948\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8947\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8947\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8946\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8945\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8945\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8944\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8943\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8942\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8941\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8941\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8940\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8939\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8939\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8938\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8937\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8937\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8936\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8936\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8935\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8934\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8934\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8933\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8933\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8932\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8931\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8931\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8930\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8929\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8929\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8928\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8928\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8927\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8926\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8926\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8925\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8924\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8924\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8923\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8923\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8922\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8922\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8921\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8921\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8920\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8919\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8919\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8918\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8918\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8917\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8917\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8916\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8916\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8915\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8915\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8914\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8914\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8913\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8913\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8912\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8912\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8911\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8911\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8910\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8910\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8909\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8909\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8908\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8908\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8907\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8907\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8907\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8906\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8906\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8905\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8905\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8904\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8904\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8903\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8903\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8902\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8902\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8901\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8901\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8899\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8899\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8898\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8897\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8897\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8897\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8896\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8896\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8895\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8895\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8894\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8894\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8894\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8893\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8893\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8892\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8892\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8892\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8891\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8891\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8890\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8890\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8889\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8889\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8889\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8888\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8888\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8887\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8887\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8886\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8886\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8885\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8885\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8885\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8884\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8884\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8884\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8883\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8883\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8883\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8882\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8882\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8881\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8881\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8880\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8880\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8880\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8879\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8879\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8878\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8878\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8878\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8877\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8877\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8876\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8876\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8875\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8875\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 0.8874\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8873\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8872\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8872\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8871\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8870\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8870\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8869\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8869\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8868\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8868\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8867\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8867\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8866\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8866\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8865\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8865\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8864\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8862\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8862\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8861\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8861\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8860\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8860\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8859\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8858\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8858\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8857\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8856\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8856\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8855\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8855\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8854\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8854\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8853\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8853\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8852\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8852\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8851\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8851\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8850\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8850\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8850\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8849\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8849\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8848\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8848\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8847\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8847\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8846\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8846\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8845\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8845\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8844\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8844\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8843\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8843\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8842\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8842\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8841\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8841\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8840\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8840\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8839\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8838\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8838\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8837\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8837\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8836\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8835\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8835\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8834\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8834\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8833\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8832\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8832\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8831\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8831\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8830\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8830\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8830\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8829\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8829\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8828\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8828\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8827\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8827\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8826\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8826\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8824\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8824\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8823\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8823\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8822\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8822\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8822\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8821\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8821\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8820\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8820\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8819\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 0.8819\n",
      "Trained model on 660 epochs for time 30.00113868713379 secs (21.999164994462223 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0325406645388389"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(compdiags, get_expdec_smallstart_three_layer_m(compdiags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from nn_compdiags_unscaled_ds.csv...\n",
      "Shape of dataset: (30248, 245)\n"
     ]
    }
   ],
   "source": [
    "unscaled = load_dataset(\"nn_compdiags_unscaled_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 7us/sample - loss: 32.5350\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 31.5530\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 30.6129\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 29.7129\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 28.8510\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 28.0256\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 27.2349\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 26.4775\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 25.7519\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 25.0565\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 24.3902\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 23.7515\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 23.1394\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 22.5526\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 21.9900\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 21.4507\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 20.9336\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 20.4377\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 19.9621\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 19.5061\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 19.0686\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 18.6491\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 18.2467\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 17.8606\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 17.4902\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 17.1349\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 16.7940\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 16.4670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 16.1531\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 15.8520\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 15.5630\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 15.2857\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 15.0195\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 14.7641\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 14.5189\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 14.2837\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 14.0578\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 13.8411\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 13.6330\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 13.4333\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 13.2415\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 13.0575\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 12.8808\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 12.7111\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 12.5482\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 12.3919\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 12.2418\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 12.0976\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.9592\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.8263\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.6987\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 11.5762\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.4585\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.3455\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 11.2370\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 11.1328\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 11.0328\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.9367\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.8444\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.7557\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.6706\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 10.5888\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 10.5103\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.4348\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.3624\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.2928\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 10.2259\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 10.1617\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 10.1000\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 10.0408\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.9839\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.9292\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.8766\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.8262\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.7777\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.7311\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.6863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.6434\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.6020\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.5624\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.5242\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.4876\n",
      "Train on 19358 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.4524\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.4185\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.3860\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.3548\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.3248\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.2959\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.2682\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.2415\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.2159\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 9.1913\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 9.1677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.1450\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.1231\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.1021\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.0819\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.0626\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.0439\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 9.0260\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 9.0088\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.9922\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.9763\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.9610\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.9463\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.9321\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.9186\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.9055\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.8929\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.8808\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.8692\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.8581\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.8473\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.8370\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.8271\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.8175\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.8083\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7995\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7910\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7828\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7750\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7602\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7532\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7465\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7400\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7338\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7278\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7220\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7165\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7112\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.7060\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.7011\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6964\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6918\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6874\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6832\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6791\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.6752\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6714\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6643\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6610\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6577\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6546\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6516\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6487\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6459\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6433\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6407\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6382\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6358\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6335\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6313\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6292\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6271\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6251\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6232\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6214\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6196\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6179\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6163\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6147\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6132\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.6103\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6089\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6076\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6063\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6051\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6039\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.6028\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6017\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.6006\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5996\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5986\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5977\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5968\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5959\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5950\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5942\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5934\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5926\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5919\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5912\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5905\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5898\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5892\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5886\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5880\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5874\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5868\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5863\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5858\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5853\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5848\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5843\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5838\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5834\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5830\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5825\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5821\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5818\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5814\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5810\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5807\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5803\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5800\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5797\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5794\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5791\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5788\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5785\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5782\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5780\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5777\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5775\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5772\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5770\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5768\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5765\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5763\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5761\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5759\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5757\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5755\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5753\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5752\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5750\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5748\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5747\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5745\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5743\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5742\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5740\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5739\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5738\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5736\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5735\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5734\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5732\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5731\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5730\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5729\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5728\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5726\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5725\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5723\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5722\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5721\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5720\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5719\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5718\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5718\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5717\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5716\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5715\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5714\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5713\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5713\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5712\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5711\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5710\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5710\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5709\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5708\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5708\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5707\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5706\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5706\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5705\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5704\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5704\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5703\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5702\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5702\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5701\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5701\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5700\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5700\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5699\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5699\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5698\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5698\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5697\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5697\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5696\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5696\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5695\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5695\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5694\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5694\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5693\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5693\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5693\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5692\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5692\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5691\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5691\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5691\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5690\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5690\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5689\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5689\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5689\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5688\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5688\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5688\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5687\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5687\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5686\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5686\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5686\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5685\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5685\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5685\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5684\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5684\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5684\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5684\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5683\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5683\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5683\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5682\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5682\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5682\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5681\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5681\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5681\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5680\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5680\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5680\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5679\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5678\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5677\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5676\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5676\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5676\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5676\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5675\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5675\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5675\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5675\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5674\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5673\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5673\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5673\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5673\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5672\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5672\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5672\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5672\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5672\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5671\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5670\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5669\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5668\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5667\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5666\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5665\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5664\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5663\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5662\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5661\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 3us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5660\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5659\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5658\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5657\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5656\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5655\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5654\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5653\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5652\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5652\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 1us/sample - loss: 8.5652\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 0s 2us/sample - loss: 8.5652\n",
      "Trained model on 505 epochs for time 30.046488285064697 secs (16.807288599214502 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.219147313971652"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(unscaled, get_perceptron_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 186us/sample - loss: 10.4329 - val_loss: 10.1571\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 171us/sample - loss: 9.2946 - val_loss: 10.0305\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 169us/sample - loss: 8.7472 - val_loss: 9.8077\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 162us/sample - loss: 8.3188 - val_loss: 11.1489\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: 8.4198 - val_loss: 9.6844\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: 8.7892 - val_loss: 12.8924\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: 9.0835 - val_loss: 69.4999\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 153us/sample - loss: 8.9857 - val_loss: 9.5915\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: 8.1909 - val_loss: 16.3265\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 153us/sample - loss: 8.7192 - val_loss: 14.1542\n",
      "Trained model on 10 epochs for time 31.872532606124878 secs (0.3137497770753969 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error: 14.15422929352161'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_two_layer_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 4s 188us/sample - loss: 28.8529\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 178us/sample - loss: 190.4860\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 16.6647\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 177us/sample - loss: 21.1203\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 178us/sample - loss: 25.6394\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 3s 178us/sample - loss: 26.1774\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 5s 234us/sample - loss: 24.9623\n",
      "Train on 19358 samples\n",
      "19358/19358 [==============================] - 5s 254us/sample - loss: 22.7561\n",
      "Trained model on 8 epochs for time 30.528220415115356 secs (0.26205261529227497 epochs in second)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.428638411360662"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(unscaled, get_four_layer_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 184us/sample - loss: 9.6874 - val_loss: 9.4124\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 174us/sample - loss: 8.7373 - val_loss: 9.4705\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 173us/sample - loss: 8.4346 - val_loss: 9.3226\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 176us/sample - loss: 8.1125 - val_loss: 9.4635\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 172us/sample - loss: 7.7450 - val_loss: 10.0134\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 173us/sample - loss: 7.3482 - val_loss: 10.0451\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 176us/sample - loss: 6.9403 - val_loss: 10.3786\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 176us/sample - loss: 6.4301 - val_loss: 10.4232\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 188us/sample - loss: 6.0047 - val_loss: 11.4130\n",
      "Trained model on 9 epochs for time 31.334738731384277 secs (0.2872211597853781 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error: 11.413037127017102'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_expdec_three_layer_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 28us/sample - loss: 28.2550 - val_loss: 17.6431\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 17.5307 - val_loss: 13.5870\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 13.9963 - val_loss: 14.0030\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 14.7077 - val_loss: 13.2746\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 13.6948 - val_loss: 11.5384\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 11.4850 - val_loss: 10.3307\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 9.9089 - val_loss: 9.9589\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 9.3642 - val_loss: 10.0377\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 9.3521 - val_loss: 10.1603\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: 9.4215 - val_loss: 10.1466\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: 9.3721 - val_loss: 10.0023\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 9.1984 - val_loss: 9.8156\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: 8.9842 - val_loss: 9.6755\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 8.8201 - val_loss: 9.6246\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 8.7507 - val_loss: 9.6434\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 8.7565 - val_loss: 9.6683\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.7735 - val_loss: 9.6398\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.7417 - val_loss: 9.5405\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.6434 - val_loss: 9.3995\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.5064 - val_loss: 9.2665\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 8.3780 - val_loss: 9.1791\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.2944 - val_loss: 9.1450\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.2611 - val_loss: 9.1475\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 8.2584 - val_loss: 9.1575\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 8.2564 - val_loss: 9.1528\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 8.2329 - val_loss: 9.1281\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 8.1825 - val_loss: 9.0935\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.1160 - val_loss: 9.0661\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 8.0517 - val_loss: 9.0591\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 8.0042 - val_loss: 9.0728\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.9756 - val_loss: 9.0961\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.9563 - val_loss: 9.1134\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.9320 - val_loss: 9.1149\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.8936 - val_loss: 9.1018\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.8431 - val_loss: 9.0840\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.7907 - val_loss: 9.0724\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.7470 - val_loss: 9.0717\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.7158 - val_loss: 9.0786\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 7.6932 - val_loss: 9.0857\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 7.6706 - val_loss: 9.0874\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 7.6418 - val_loss: 9.0828\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.6059 - val_loss: 9.0757\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.5676 - val_loss: 9.0709\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.5324 - val_loss: 9.0704\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.5030 - val_loss: 9.0717\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.4772 - val_loss: 9.0704\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.4502 - val_loss: 9.0640\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 7.4194 - val_loss: 9.0539\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: 7.3858 - val_loss: 9.0440\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 26us/sample - loss: 7.3529 - val_loss: 9.0376\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 39us/sample - loss: 7.3236 - val_loss: 9.0349\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 7.2975 - val_loss: 9.0338\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.2715 - val_loss: 9.0323\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.2432 - val_loss: 9.0304\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.2123 - val_loss: 9.0294\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 7.1807 - val_loss: 9.0311\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 7.1508 - val_loss: 9.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: 7.1229 - val_loss: 9.0392\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: 7.0955 - val_loss: 9.0417\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.0670 - val_loss: 9.0424\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: 7.0371 - val_loss: 9.0423\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: 7.0071 - val_loss: 9.0428\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.9781 - val_loss: 9.0445\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.9501 - val_loss: 9.0464\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 6.9217 - val_loss: 9.0480\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.8922 - val_loss: 9.0495\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 6.8619 - val_loss: 9.0517\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.8317 - val_loss: 9.0547\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.8022 - val_loss: 9.0580\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 18us/sample - loss: 6.7731 - val_loss: 9.0605\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 19us/sample - loss: 6.7434 - val_loss: 9.0621\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 34us/sample - loss: 6.7131 - val_loss: 9.0634\n",
      "Trained model on 72 epochs for time 30.666459798812866 secs (2.3478419247723927 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.063432207515444"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_expdec_three_layer_m(unscaled), batch_size=unscaled.X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 104us/sample - loss: 9.7329 - val_loss: 9.4522\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 94us/sample - loss: 8.6732 - val_loss: 9.4169\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 94us/sample - loss: 8.2365 - val_loss: 9.4075\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 93us/sample - loss: 7.9018 - val_loss: 9.5372\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 98us/sample - loss: 7.4684 - val_loss: 9.8459\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 98us/sample - loss: 7.0787 - val_loss: 10.4999\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 96us/sample - loss: 6.6184 - val_loss: 10.1360\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 95us/sample - loss: 6.2067 - val_loss: 10.6056\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 95us/sample - loss: 5.7057 - val_loss: 10.9054\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 93us/sample - loss: 5.2936 - val_loss: 11.3744\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 95us/sample - loss: 4.9503 - val_loss: 11.3313\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 105us/sample - loss: 4.4684 - val_loss: 12.1467\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 112us/sample - loss: 4.1236 - val_loss: 12.4145\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 105us/sample - loss: 3.8001 - val_loss: 12.3344\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 106us/sample - loss: 3.5556 - val_loss: 12.6217\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 103us/sample - loss: 3.2459 - val_loss: 12.1110\n",
      "Trained model on 16 epochs for time 31.470797061920166 secs (0.5084078413558862 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.11095078437917"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_expdec_three_layer_m(unscaled), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying another model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 121us/sample - loss: 37.0973 - val_loss: 20.4611\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 116us/sample - loss: 22.1584 - val_loss: 20.6889\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 113us/sample - loss: 17.7216 - val_loss: 12.6337\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 11.8790 - val_loss: 14.3453\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 13.0135 - val_loss: 11.4855\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 111us/sample - loss: 9.9532 - val_loss: 10.6038\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 9.8279 - val_loss: 10.3664\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 9.2100 - val_loss: 9.7331\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 8.7782 - val_loss: 10.0068\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 110us/sample - loss: 8.7949 - val_loss: 9.4387\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 128us/sample - loss: 8.4347 - val_loss: 9.3814\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 135us/sample - loss: 8.4291 - val_loss: 9.2423\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 147us/sample - loss: 8.2498 - val_loss: 9.3052\n",
      "Trained model on 13 epochs for time 30.12335228919983 secs (0.43155887416490857 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.30521865385334"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxcdb3/8ddnJpO9bZYmbdrSJd03aGlsS+tVqWyCVxYBL1cBvXgRVHBX1OvC/V25KIrovYqySdGKcAEFRFGQTaUtTWspbVPa0n0hSZukSZs9+f7+OJN02qbNJJlkcmbezwfzmDNnzjnzObR9z5nv+Z7vMeccIiLiP4F4FyAiIr2jABcR8SkFuIiITynARUR8SgEuIuJTKQP5YcOHD3fjx48fyI8UEfG91atXH3DOFRw/f0ADfPz48ZSWlg7kR4qI+J6Z7exqvppQRER8SgEuIuJTCnAREZ9SgIuI+JQCXETEpxTgIiI+pQAXEfEpXwT4i29W8NOXtsa7DBGRQcUXAb78rYPc9dwWGlva4l2KiMig4YsAX1icR3NbO//YVRPvUkREBg1fBHjJ+DwCBiu3H4x3KSIig4YvAnxoeoiZo4axYpsCXESkgy8CHGDBhDz+satG7eAiImH+CfDifJpa23l9t9rBRUTARwE+f3weZrBye1W8SxERGRR8E+DDMkNMHzlU7eAiImG+CXCAhcX5rNlVTVOr2sFFRHwV4AuK82hsaWfdnkPxLkVEJO78FeATwu3gakYREek+wM0s3cxeM7PXzWyDmd0anv+gmW03s7Xhx5z+LjYnM5WpI4boRKaICNHd1LgJWOKcO2xmIeBvZvbH8Htfcs491n/lnWhhcT6PrNpNS1s7oaCvfkCIiMRUtwnoPIfDL0Phh+vXqk5hYXEeDS1tagcXkaQX1SGsmQXNbC1QATznnFsZfus7ZrbOzH5oZmn9VmWE+RPyAdSdUESSXlQB7pxrc87NAcYA881sFvBVYBrwDiAP+EpX65rZ9WZWamallZWVfS44L0vt4CIi0MNeKM65GuAl4ALn3P5w80oT8Atg/knWucc5V+KcKykoKOhzweB1JyzdUUVLW3tMtici4kfR9EIpMLOc8HQGcA6wycyKwvMMuARY35+FRlpYnE99cxvr96odXESSVzS9UIqApWYWxAv8R51zvzezF8ysADBgLXBDP9Z5jPkT8gBvXJS5Y3MH6mNFRAaVbgPcObcOmNvF/CX9UlEUhmenMakwmxXbDnLDuyfGqwwRkbjybUfqhcV5lO6oplXt4CKSpHwb4Asm5HO4qZUN+2rjXYqISFz4N8CLO9rB1R9cRJKTbwO8cEg6xQVZrNim/uAikpx8G+DgdSdctb2Ktva4XdkvIhI3vg7wBRPyqGtqpWy/2sFFJPn4OsAXFmtcFBFJXr4O8BFD05kwXO3gIpKcfB3g4DWjvLb9oNrBRSTp+D7AFxbnU9vYyqa31Q4uIsnF9wHe0R9czSgikmx8H+BFwzIYl5+pGx2LSNLxfYBDuB18RxXtagcXkSSSIAGeT019C2+W18W7FBGRAZMYAd4xLoqaUUQkiSREgI/JzWRMboZOZIpIUkmIAAevO6HawUUkmSRMgC+YkEfVkWa2VByOdykiIgMiYQK8Y1wUjQ8uIskiYQL8tLxMRudkaGArEUkaCRPg0DEuShXOqR1cRBJfQgX4wuJ8Dhxu5q1KtYOLSOJLqADv6A++XN0JRSQJJFSAj83LpGhYui7oEZGkkFABbmYsmJDHim1qBxeRxJdQAQ4d7eBNbDtwJN6liIj0q24D3MzSzew1M3vdzDaY2a3h+RPMbKWZbTGzR8wstf/L7d4C3SdTRJJENEfgTcAS59wZwBzgAjNbCHwX+KFzbjJQDVzXf2VGb3x+JoVD0lipE5kikuC6DXDn6eiXFwo/HLAEeCw8fylwSb9U2ENmxsLifFZuP6h2cBFJaFG1gZtZ0MzWAhXAc8BbQI1zrjW8yB5g9EnWvd7MSs2stLKyMhY1d2tBcR7ltU3sOFg/IJ8nIhIPUQW4c67NOTcHGAPMB6Z3tdhJ1r3HOVfinCspKCjofaU90DkuitrBRSSB9agXinOuBngJWAjkmFlK+K0xwL7YltZ7xcOzGJ6dphOZIpLQoumFUmBmOeHpDOAcoAx4Ebg8vNi1wJP9VWRPmRkLivNYqXFRRCSBRXMEXgS8aGbrgFXAc8653wNfAT5vZluBfOD+/iuz5xYW57P/UCO7qtQOLiKJKaW7BZxz64C5XczfhtcePigtnNBxn8wqxuVnxbkaEZHYS7grMTtMKswmPyuVFbrBg4gkqIQN8M52cF3QIyIJKmEDHLx28L01DexWO7iIJKCEDvAFEzQuiogkroQO8MmF2eRmhli5Xc0oIpJ4EjrAAwFjwYR8HYGLSEJK6AAHb1yUPdUN7KlWO7iIJJaED/Cj46KoGUVEEkvCB/jUEUPIyQyxUv3BRSTBJHyABwLG/PF5OpEpIgkn4QMcvNus7TxYz/5DDfEuRUQkZpIjwCPGRRERSRRJEeDTi4YyND1F3QlFJKEkRYAHA8b8CWoHF5HEkhQBDl53wu0HjlBe2xjvUkREYiJpAlzjoohIokmaAJ8xaihD0lLUjCIiCSNpAjwYMN4xIU9H4CKSMJImwMHrTrit8ggVdWoHFxH/S6oA17goIpJIkirAZ44aSnZaisZFEZGEkFQBnhIMUDI+lxU6AheRBJBUAQ5ed8KtFYc5cLgp3qWIiPRJ0gX4wmJvXJTX1J1QRHwu6QJ81uhhZKYG1Z1QRHyv2wA3s9PM7EUzKzOzDWb2mfD8b5vZXjNbG35c2P/l9l0oGGDeuFz1RBER34vmCLwV+IJzbjqwEPiUmc0Iv/dD59yc8OMP/VZljC0szufN8jqqjjTHuxQRkV7rNsCdc/udc2vC03VAGTC6vwvrT0fbwdWMIiL+1aM2cDMbD8wFVoZnfdrM1pnZA2aWe5J1rjezUjMrrays7FOxsTJ7dA4ZoaC6E4qIr0Ud4GaWDTwOfNY5VwvcDUwE5gD7gR90tZ5z7h7nXIlzrqSgoCAGJfddaorXDq4TmSLiZ1EFuJmF8MJ7mXPuCQDnXLlzrs051w7cC8zvvzJjb2FxHpverqNa7eAi4lPR9EIx4H6gzDl3Z8T8oojFLgXWx768/rMgPC7KazvUjCIi/pQSxTKLgauBN8xsbXje14CrzGwO4IAdwCf6pcJ+cvqYYaSlBFi5rYrzZ46MdzkiIj3WbYA75/4GWBdv+abbYFfSUoJqBxcRX0u6KzEjLZiQT9nbtRyqb4l3KSIiPZbUAb6wOA/n1A4uIv6U1AF+xmk5pKYEWKlmFBHxoaQO8PRQkDPH5rBCV2SKiA8ldYCD1w6+cV8ttY1qBxcRf1GAF+fR7qBU7eAi4jNJH+Bnjs0lNRjQuCgi4jtJH+DpoSBzTsvRiUwR8Z2kD3DwuhO+sfcQdWoHFxEfUYDjjYvS7qB0Z3W8SxERiZoCHK8dPBQ0XVYvIr6iAAcyUoOcMSZH98kUEV9RgIctCLeDH2lqjXcpIiJRUYCHnVU8nLZ2x29W7Y53KSIiUVGAhy2amM850wu57Q9l/HXL4Lh3p4jIqSjAwwIB465/mcvkwmw+uWwNb1UejndJIiKnpACPkJ2Wwr3XlJAaDPDxpaXU1Ot+mSIyeCnAj3NaXiY/v3oee6sb+OSyNbS0tce7JBGRLinAu1AyPo//vmw2r751kG8/tQHnXLxLEhE5QTQ3NU5KH5w3hi0Vh/nZy28xZcQQrl00Pt4liYgcQ0fgp/Dl86dyzvQR3Pr0Bl7ZrJ4pIjK4KMBPweuZMocpI4bwqV+vYWuFeqaIyOChAO/Q1grN9SfMzk5L4b5rS0hLCfDxpavUM0VEBo3EaQNvbYLGWmg8FH7UeM9NkfMOHbfMoaPLNB8GDK59Cia865hNj8n1eqZcdc9KbvzVGh66bj6hoL77RCS+/BHgbz4Lu5Z3Hbwd062Np96GBSF9KKQPO/rInwjpOeHXQ6H0F/DK908IcIB54/K4/YOz+fyjr/PNJzdw26WzMLN+2mERke51G+BmdhrwEDASaAfucc79yMzygEeA8cAO4ErnXP8MqP3WC7D6F8eGb/owGDbmxHmRj7SIwE7Ngu4CN5QBz30T9q2FUXNOePuyM72eKXe/9BZTRmTzscUT+mV3RUSiYd31cTazIqDIObfGzIYAq4FLgI8CVc65283sFiDXOfeVU22rpKTElZaW9rzK9jYIBHu+Xk81HoI7Z8KU8+DyB7oupd1xw69W83xZOQ989B28Z2ph/9clIknNzFY750qOn99tQ65zbr9zbk14ug4oA0YDFwNLw4stxQv1/jEQ4Q3ekXrJx2DDb6F6R9elBIwffmgOU0cO5aZf/4OtFXUDU5uIyHF6dCbOzMYDc4GVwAjn3H7wQh7o8lDUzK43s1IzK62s9EFf6oU3eu3ly3960kWyOnqmhAJct7SU6iPqmSIiAy/qADezbOBx4LPOudpo13PO3eOcK3HOlRQUFPSmxoE1dBScfiWseQiOnPwWa6NzMvj51SXsr2nkxmWraW7VmCkiMrCiCnAzC+GF9zLn3BPh2eXh9vGOdvKK/ikxDhbdBK0NsOq+Uy42b1wu3718Niu2VfGtp9ZrzBQRGVDdBrh5feXuB8qcc3dGvPUUcG14+lrgydiXFyeF02Hy+fDaz6Gl4ZSLXjp3DJ98z0Qefm03v/j7joGpT0SE6I7AFwNXA0vMbG34cSFwO3CumW0Bzg2/ThyLPwP1B2Htsm4X/eJ5Uzlvxgj+65mNvPhm4vwQEZHBrdtuhLHU626E8eAc3HeOF+I3re62J8yRplYu/9ly9lTV88QnFzF5xJABKlREEl2vuxEmLTNYfDNUb4eyp7td/GjPlCDXLS2lSj1TRKSfKcBPZdr7Ia8Y/v4j74i8G6NzMrjnmnm8XdvIjb9SzxQR6V8K8FMJBOGsT8O+NbDz71GtcubYXL73wdNZub2Kb/xOPVNEpP8owLsz518hc7h3FB6lS+aO5tNnT+KR0t3c/7ft/ViciCQzBXh3Qhmw4AbY8mco3xj1ap8/dwrnzxzBbX8o48VN6pkiIrGnAI/GO66DUCa8+j9Rr9IxZsq0kUO56eF/sLlcY6aISGwpwKORmQdnXgNvPAqH9ka/WqrXMyU9FOS6pavUM0VEYkoBHq2Fn/R6oqw4+SBXXRmVk8G918yjvLaJG9QzRURiSAEerdxxMPNSWL0UGmp6tOrcsbnccfnpvLa9iv/43RvqmSIiMaEA74nFN0NznXd3oB66eM5obloyiUdL9/DgqztiX5uIJB0FeE8UnQHFZ8OKn3k3Ue6hz50zhSXTCvnus5vYdbC+HwoUkWSiAO+pxTfD4bdh3aM9XjUQML5z6SxSAgG+rqYUEekjBXhPFZ8NI2fDqz+G9p6fkCwalsGXL5jKX7cc4Mm1+/qhQBFJFgrwnjKDRZ+BA5thy596tYkPLxjH3LE5/OfvN6proYj0mgK8N2ZeAsPG9ujy+kjBgPHfl82mtqGF7zxTFuPiRCRZKMB7IxiCsz4Fu5bD7td6tYlpI4fyiXcX8/iaPfx964EYFygiyUAB3ltzPwLpOb0+Cge4aclkxudn8rXfvkFjS1sMixORZKAA7620bJj/77DpGTiwtVebSA8Fue2y2ew8WM+P/7IlxgWKSKJTgPfF/E9AMBWWRz/I1fEWTRzOFfPGcM8r2yjbXxvD4kQk0SnA+yK7wBsvfO3DUFfe68187cLpDMsIccsTb9DWrr7hIhIdBXhfLboJ2prhtZ/3ehO5Wal8859n8PruGn65fEfMShORxKYA76v8iTD9/bDqPmg63OvNfOCMUbxrSgF3/OlN9tU0xLBAEUlUCvBYWPxZaDwEax7q9SbMjO9cMot2B998UvfSFJHuKcBjYUwJjFsMy38CbS293sxpeZl8/twpPF9WwbPr345hgSKSiBTgsbLoZqjdAxt+26fNfGzxeGaOGso3n9rAoYbefxmISOJTgMfK5POgYJp3YU8fmj9SggFuv+x0Dh5u4rvPbophgSKSaLoNcDN7wMwqzGx9xLxvm9leM1sbflzYv2X6QCDgHYWXr4e3XujTpmaPGca/LZ7Ar1fuYtWOqhgVKCKJJpoj8AeBC7qY/0Pn3Jzw4w+xLcunZl8BQ4r6dHl9h8+dO4XRORl89Yk3aGrVZfYicqJuA9w59wqgw8BopKTCwhth+8uwb22fNpWVlsJ/XTqLrRWH+dlL22JUoIgkkr60gX/azNaFm1hyT7aQmV1vZqVmVlpZWdmHj/OJeR+FtKHeDR/66OyphXzgjFH85MWtbK3ofR9zEUlMvQ3wu4GJwBxgP/CDky3onLvHOVfinCspKCjo5cf5SPowL8Q3/Baqd/R5c994/wwyUoN87Yk3aNdl9iISoVcB7pwrd861OefagXuB+bEty+cW3ggW9PqF91HBkDS+fuF0XttRxSOlu2NQnIgkil4FuJkVRby8FFh/smWT0tBRcPqVsOaXcORgnzd3RckYFhbncdsfyqiobTz1wu1tsOF38PL3oEWX5Isksmi6ET4MLAemmtkeM7sO+J6ZvWFm64Czgc/1c53+s+gmaG3wxkjpIzPjtktn09Tazq2/39j1Qi0NsOp++J958H/XwovfgfvPhSqdABVJVNH0QrnKOVfknAs558Y45+53zl3tnJvtnDvdOfcB59z+gSjWVwqnw5QLvFEKY3AkXFyQzc1LJvHMuv38pSxi6NqGanjlDrhrNjzzecjIhSsfgqt+AzW74efvgU3q5SmSiHQlZn9adDPUH4S1y2KyuevfNZEpI7L5xu/Wc6RyJzz7NbhzJrzwX1A0Bz76DPz7CzDjYpj6PvjEy5A3Hn5zFTx/K7S1xqQOERkcUuJdQEIbtwhGl8Cr/wvzPgaBYJ82l5oS4Idnp7Hp8TtI/8mrYMDsy70vipGzTlwhdzz825/h2a/A3+6EvaXwwQe8G1GIiO/pCLw/mcHiz0D1dih7uvfbcQ52/A2WXcHM353P+0OlPNh6HhuueAUuu6fr8O4QSod//hFc/FPY/Rr8/J9g14re1yIig4YCvL9Nuwjyins3yFV7G2x8Cu47Bx68CPaugbP/g6ab1nFP5sf54nPVtLS1R7etuR+G656DlHRvWyvu7tOgWyISfwrw/hYIej1S9q3xjqKj0dIIqx+En8yHR6+G+gNw0Q/gc+vh3V9iaG4h/3nxLMr213L/37ZHX0vR6XD9S97Iic/eAo99DJrqerFTIjIYKMAHwhlXQebw7i+vb6iBv94JPzodnv4MpGbB5b+Am9bAOz4OoYzORc+fOZLzZ47gruc3s/PgkehryciBDy2Dc74NG5+Ee5dAhYatFfEjBfhACGXAghtgy5+hvIt+3If2wp++Dj+cCX+5FUbMhGuehOtfhlmXnfTk560fmEVKIMDXf9vDW7AFAvDOz3mf0VDthfgbj0W9unOOzeV1PLJqF5V1TdF/rojElA3kvRdLSkpcaWnpgH3eoFJf5QX0jEvg0ru9eRWbvKPydY+Ca4OZl3knPYtOj3qzv1y+g288uYE7rzyDy84c0/O6avfD/30Udq+A+dfDed/xRlU8TmtbO6t2VPN8WTnPl5Wz82A9AEXD0rn3mhJmjR7W888WkaiY2WrnXMkJ8xXgA+iPX/GuzLz8AVj7a9j8LKRkwJlXw1mf8rr99VB7u+Pyn73K9gNH+MsX3kNe1onh2622FnjuW7DiJzDmHXDFgzBsDIebWnllcyXPbSznhU0VHGpoITUlwOKJ+ZwzYwTj8rL48mOvU13fwg+uPIMLZxd1+1Ei0nMK8MGgeif8eK53tJ2R5x3xzr8esvL7tNnN5XVc9OO/8s9njOLOK+f0fkMbfkv77z5FM6n8KPcW7t87jua2dnIyQyyZVsh5M0bwT5MLyEo7evlARV0jN/xyNWt21fDZcyZz85LJBALWp/0RkWMpwAeLNb+E1kaY86/eScoY+cGf3+R/XtjKr65bwDsnD496PeccZfvreG6j1zRSv28jd4fuYlJgH6+Mvp6MJV9i3vh8UoInP13S1NrG155Yz+Nr9nDR7CK+f8UZZKT27aIlETlKAZ7gGlvaeN+P/kpbu+NPn33XKQO0pa2dlduqeL6snOc2lrO3pgEzmHtaDufOGMl5k7MoXv51bP1jMPl8uOzn3hgrp+Cc496/buO//7iJGUVDufeaEkblZJxyHRGJjgI8CSx/6yBX3buCG949kVveN+2Y9w41tPByuD37pTcrqGtsJT0U4J2TCjh3RiFLpo2gYEja0RWc89rrn/0qDC2CK38Jo7pvnnlhUzk3P7yW9FCQe66Zx5ljTx38ItI9BXiS+PJjr/P4mr08/el3MjQjhec3lvN8WQUrth2ktd2Rn5XKe6cXcu6Mkbxz0vDumzp2r/KGpz1yAC68A868xhsi4BS2lNdx3dJS3q5t5PbLZveud4yIdFKAJ4ma+mbOufNl6pvbqG/27mY/sSCLc2eM5NwZhcw5LZdgT08yHjkAj38ctr0Icz4CF33/mIuKulJ9pJkbl61mxbYqbnj3RL50/tSef66IAArwpPLipgoe+Pt2/mnycM6ZPoLiguy+b7S9DV66HV75HoyYDR96yBvj5RRa2tq59ekN/GrFLt47rZC7/mUOQ9JDfa9FJMkowCU2Nv8JnrjeayO/9G5vsK5u/HL5Dr799EYmFmRx3zXvYGx+Zv/XKZJAFOASO9U74dFrYP9aKJwBY8/yxj4ft8i7H2gX/r71AJ9ctoaAwU8/PI+zJvat77tIMlGAS2y1NHq3i9v2kjfOePNhb37OOC/IO0I9f1LnSc8dB45w3dJV7DxYz39ePIt/XTA2fvWL+IgCXPpPWyuUvwE7l8OuV73n+gPee1kFMHYhjF0E486iNmcaNz/yBi+9WclHF43nPy6afsqLhEREAS4DyTk4uBV2vgq7lnvPNTu991KzcWPm83LTJO7eNoLM4vnc9eGzGJapk5siJ6MAl/iq3RcR6MuhYiPgaHZBNgcnM/qM95I7/d1w2gJvzHIR6aQAl8GloRp2rWT/Gy9Qsf5FZvAWIdoA88ZDH3sWjDvLa3oZqlEOJbkpwGXQ2lNdz6eX/p2MirV8cVoVZ1KG7X4NWsJ3GkofFn7keM8ZORGvj5933HIp6d1eOSoy2J0swFO6WlhkII3JzWTZjWfzhUfz+OCGt/lQyVX8vy9NI7VyvdfkUr0TGg9BY433fGDr0dct9afeeDD1JOF/3LzM4VAwFXInQFD/LMQfuv2bamYPAO8HKpxzs8Lz8oBHgPHADuBK51x1/5UpiS4rLYWffvhM7np+Mz9+YSvbDhzm7o/MY/hZZ556xdZmaKr17ifaeAgaq73nztcRwd9Q490ZqWq7N6+hxhubPVIwDQqmeP3bC6Z5z4XTYdhp3q3oRAaRbptQzOxdwGHgoYgA/x5Q5Zy73cxuAXKdc1/p7sPUhCLRePr1fXzx/15neHYa911bwvSiof3zQc5B8xEv3A+/DZVveidXKzZBRRnU7jm6bGq2d4ReOP1oqBdMhyEj1UQj/a5PbeBmNh74fUSAvwm8xzm338yKgJecc1O7244CXKK1bk8N//5QKXWNrXxs8XiGpodISwmQHgqSHgp2TqeFAqSlBEkPBY6Z3zEd6ksf88ZD4TDfCJXh54oyOFJ5dJn0nKOB3vmYAZl5ff+f0KG1GRqqvF8PJzxXh6erj84LhrwLqIZPhvzJMHyS95zeT1+E0u9iHeA1zrmciPernXNdDvxsZtcD1wOMHTt23s6dO3u1A5J8Kmob+dSv17BqR+9b54IBOxrqKQHSIsO/M+wDZKeFmFiYxZTCIUwZMYQxuRknvzXc4UqoLDsa7hVl3qPp0NFlskcc2wRTOMM7gnftJwZu5HNDdcS88DIdV7l2uYNp3pdFRl74ORdam+DgFqje4X1eZ00jvVDvDPbJXtDnjIWA7qA0mMUtwCPpCFx6o63d0dTaRmNLO40tbTS1es/HTrfT1NpGU0s7ja3h9zqn27tcvylifk1DM+W1TZ2fmR4KMKkwmymFQ5g0Irv7YHfO6+teWXY00DuaY1obothL806oHhPGkc+53nNG7rHvhTJP3oTT2uS19x/cAge2eBdXHdjsTTfWHF0umAb5E48etQ+fcvTIPX1Yz/6wpF/EuhdKuZkVRTShVPStPJGTCwaMzNQUMlP793NqG1vYUn6YLeV1bKk4zObyOl596yBP/GNv5zKRwT55xBCmjMhmcmE42IeNhmGjYdI5Rzfa3u5dhVpRBgfehEDoxHDOyPV6wsT6KDglDQqneY9IzkH9QS/ID2wOB/xWKN8Am5459sRuVqEX6B3NMB3TOeN01D4I9PYI/A7gYMRJzDzn3Je7246OwMWPDjW0sLXCC/bN5YfZUlHHlvLDvF3b2LlMRijIpMJsJhdmdwb7lBFDGJ1ziqaYwai12Wt6ORgO9wNbj043RDRlWcD7MgqkhB/BiOkYvk5Jh1C690sjlBF+HZ7ueKR0TGeeuGy8TjA7542h394C7a3eI5QFKb07Cul1E4qZPQy8BxgOlAPfAn4HPAqMBXYBVzjnqrorQgEuicQLdi/Muw328JH6uPxMRuVkMConneFZaf4K9yMHjzbH1OyEtuZwSLUe9zh+3qlet53i/RavGailwZvujZSM40I949jwD2V4X0Qdn9feBm0tXexTqzdoW+dyradetr31xFo+8vixv856QFdiigyQaIIdIBQ0ioZ5YT4qJ4PRORnhcM9gdE46RcMyyErTRUWAF56tDV6Yt9R7wxm31HuvO+d3NS/i0dX6rY3eF1Eg5PXe6fwlEDr6SyAYOu7XQUrEvOCxv0SCkctFbC8YgqkXQu64Xu2+rsQUGSDDMkLMG5fHvHHHdiWsbWxhb3UD+2q8x96axs7pFW8d5O3aRtrdidvqCPRRxwX8qJwMCoekJ8e9RoMpEBwCaUPiXcmgogAXGSBD00MMLQqd9MKk1rZ2KuqawuHewL6IgN9T3cBr26uobTz2p3kwYIwcmt55FD8qJ4NhGSGcA4ej4we2cy48D9ojpnEO5z3hcO6baTAAAAYFSURBVLS7o9Ph/zrXbY/YZsCM/OxUCoekUTg0nYLsNAqHppGXmeqvZiGfU4CLDBIpwUBnCJ/wWzmsrrGF/Yc6gr0x4mi+gTW7qnlm3X5ajz+M74YZGF4oe9PeDOt8zwgYmBkGYF7XzvrmthO2lRIwhofDvHBIGgVD0sMhn0ZhxPTw7LS+XWQlgAJcxFeGpIcYkh5iyoiumxLa2h2NLW2dYQxHQ9gsHNId8/rYQ6O+uZXKuiYq6pqoqG2ioq6Rirqmznl7qhv4x64aDh5p7nL9vKzUcMiHwz0c+pHT6aGjXRXthInwlw3HdjaJ3KuOfTx23rHrBoNGZijoy18OCnCRBBIM2ICd+MxMTWFcfgrj8rNOuVxLWzsHDneEfEfAN3YGf2VdI1srDlNZ19TjXw+xlJkaJDM1hey0jucUMtOCZKWmkNXlvBSyUoPec+T74XlpKYE+f0l2RwEuIv0qFAxQNCyDomEZp1yuvd1RXd9MZUTYN7d6QwF4LfUQ2WmuczJiZmT8R7b/H/9+5HZa2tqpb27jSFMrR8LP9c2tHG5qpepIM7ur6jnS1MaR5laONLWecKL5ZLwL0IKdoX7bpbNZUJwf3cpRUoCLyKAQCBj52WnkZ6cxbWS8q+mac46m1nYv7CNCvSP4vfBv43D4S+BIU1vnvCHpsb/vqwJcRCRKZtY52mV+dryrAZ0GFhHxKQW4iIhPKcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSnFOAiIj41oDd0MLNKoLe3pR8OHIhhOfGkfRl8EmU/QPsyWPVlX8Y55wqOnzmgAd4XZlba1R0p/Ej7Mvgkyn6A9mWw6o99UROKiIhPKcBFRHzKTwF+T7wLiCHty+CTKPsB2pfBKub74ps2cBEROZafjsBFRCSCAlxExKd8EeBmdoGZvWlmW83slnjX0xtmdpqZvWhmZWa2wcw+E++a+srMgmb2DzP7fbxr6QszyzGzx8xsU/jP56x419RbZva58N+v9Wb2sJmlx7umaJnZA2ZWYWbrI+blmdlzZrYl/JwbzxqjcZL9uCP892udmf3WzHJi8VmDPsDNLAj8BHgfMAO4ysxmxLeqXmkFvuCcmw4sBD7l0/2I9BmgLN5FxMCPgGedc9OAM/DpPpnZaOBmoMQ5NwsIAv8S36p65EHgguPm3QL8xTk3GfhL+PVg9yAn7sdzwCzn3OnAZuCrsfigQR/gwHxgq3Num3OuGfgNcHGca+ox59x+59ya8HQdXkiMjm9VvWdmY4CLgPviXUtfmNlQ4F3A/QDOuWbnXE18q+qTFCDDzFKATGBfnOuJmnPuFaDquNkXA0vD00uBSwa0qF7oaj+cc392zrWGX64AxsTis/wQ4KOB3RGv9+Dj4AMws/HAXGBlfCvpk7uALwPt8S6kj4qBSuAX4eag+8wsK95F9YZzbi/wfWAXsB845Jz7c3yr6rMRzrn94B0EAYVxricW/g34Yyw25IcAty7m+bbvo5llA48Dn3XO1ca7nt4ws/cDFc651fGuJQZSgDOBu51zc4Ej+ONn+gnC7cMXAxOAUUCWmX0kvlVJJDP7Ol5z6rJYbM8PAb4HOC3i9Rh89LMwkpmF8MJ7mXPuiXjX0weLgQ+Y2Q68Jq0lZvar+JbUa3uAPc65jl9Dj+EFuh+dA2x3zlU651qAJ4BFca6pr8rNrAgg/FwR53p6zcyuBd4PfNjF6AIcPwT4KmCymU0ws1S8kzJPxbmmHjMzw2tnLXPO3RnvevrCOfdV59wY59x4vD+PF5xzvjzSc869Dew2s6nhWe8FNsaxpL7YBSw0s8zw37f34tMTshGeAq4NT18LPBnHWnrNzC4AvgJ8wDlXH6vtDvoADzf8fxr4E95fxkedcxviW1WvLAauxjtaXRt+XBjvogSAm4BlZrYOmAPcFud6eiX8K+IxYA3wBt6/b99cim5mDwPLgalmtsfMrgNuB841sy3AueHXg9pJ9uN/gSHAc+F/+z+LyWfpUnoREX8a9EfgIiLSNQW4iIhPKcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSn/j+7XqvMnQm6QgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_three_layer_m(unscaled), batch_size=unscaled.X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 22s 1ms/sample - loss: 10.0759 - val_loss: 9.4135\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 24s 1ms/sample - loss: 8.8090 - val_loss: 10.0929\n",
      "Trained model on 2 epochs for time 46.335320472717285 secs (0.04316361642901813 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.092937119136156"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_three_layer_m(unscaled), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 13s 666us/sample - loss: 10.7479 - val_loss: 9.3627\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 13s 694us/sample - loss: 8.7236 - val_loss: 9.2474\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 15s 799us/sample - loss: 8.3140 - val_loss: 9.3848\n",
      "Trained model on 3 epochs for time 41.91525459289551 secs (0.07157298766612978 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.384808346360845"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_three_layer_m(unscaled), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_fact(tensor):\n",
    "    return tf.exp(tf.math.lgamma(tensor + 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "def my_poisson_loss(y_actual, y_predicted):\n",
    "    \n",
    "    lmbd = tf.keras.backend.cast(y_predicted, tf.keras.backend.floatx())\n",
    "    k = tf.keras.backend.cast(y_actual, tf.keras.backend.floatx())\n",
    "    \n",
    "    return lmbd - k*tf.math.log(1 + lmbd)\n",
    "\n",
    "def get_my_poisson_twol_m(dataset):\n",
    "    my_poisson_twol_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    my_poisson_twol_m.compile(optimizer=tf.keras.optimizers.Adam(), loss=my_poisson_loss)\n",
    "    \n",
    "    return my_poisson_twol_m\n",
    "\n",
    "def get_poisson_twol_m(dataset):\n",
    "    poisson_twol_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    poisson_twol_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='poisson')\n",
    "    \n",
    "    return poisson_twol_m\n",
    "\n",
    "def get_poisson_expdec_threel_m(dataset):\n",
    "    poisson_expdec_threel_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/2), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    poisson_expdec_threel_m.compile(optimizer=tf.keras.optimizers.Adam(), loss='poisson')\n",
    "    \n",
    "    return poisson_expdec_threel_m\n",
    "\n",
    "def get_my_poisson_expdec_threel_m(dataset):\n",
    "    my_poisson_expdec_threel_m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1], input_shape=(dataset.X_train.shape[1],), activation='relu'),\n",
    "        tf.keras.layers.Dense(simple.X_train.shape[1]**(1/2), activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    my_poisson_expdec_threel_m.compile(optimizer=tf.keras.optimizers.Adam(), loss=my_poisson_loss)\n",
    "    \n",
    "    return my_poisson_expdec_threel_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 32us/sample - loss: 15.7786 - val_loss: 0.5093\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -0.0152 - val_loss: -1.3761\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -1.3866 - val_loss: -1.8698\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -1.8286 - val_loss: -2.0807\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.0318 - val_loss: -2.1907\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.1381 - val_loss: -2.2489\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.2075 - val_loss: -2.3227\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.2637 - val_loss: -2.3719\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.2789 - val_loss: -2.3797\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.2848 - val_loss: -2.3841\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.2895 - val_loss: -2.3875\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.2946 - val_loss: -2.3905\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3000 - val_loss: -2.3698\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3053 - val_loss: -2.3734\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3106 - val_loss: -2.3763\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3157 - val_loss: -2.3790\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 26us/sample - loss: -2.3206 - val_loss: -2.3814\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 31us/sample - loss: -2.3249 - val_loss: -2.3837\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 25us/sample - loss: -2.3290 - val_loss: -2.3855\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 37us/sample - loss: -2.3328 - val_loss: -2.3871\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3361 - val_loss: -2.3884\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3390 - val_loss: -2.3894\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3418 - val_loss: -2.3828\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: -2.3444 - val_loss: -2.3838\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 25us/sample - loss: -2.3469 - val_loss: -2.3848\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 34us/sample - loss: -2.3492 - val_loss: -2.3860\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: -2.3515 - val_loss: -2.3871\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3537 - val_loss: -2.3880\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3557 - val_loss: -2.3892\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3576 - val_loss: -2.3902\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3592 - val_loss: -2.3906\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3609 - val_loss: -2.3913\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3625 - val_loss: -2.3919\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: -2.3641 - val_loss: -2.3920\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: -2.3657 - val_loss: -2.3922\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3671 - val_loss: -2.3924\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3652 - val_loss: -2.3929\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3668 - val_loss: -2.3933\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3683 - val_loss: -2.3938\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: -2.3697 - val_loss: -2.3939\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3710 - val_loss: -2.4013\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3722 - val_loss: -2.4019\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3735 - val_loss: -2.4020\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: -2.3778 - val_loss: -2.4030\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3788 - val_loss: -2.4038\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3799 - val_loss: -2.4049\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 26us/sample - loss: -2.3808 - val_loss: -2.4053\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3819 - val_loss: -2.4047\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3831 - val_loss: -2.3978\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: -2.3845 - val_loss: -2.3984\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3859 - val_loss: -2.3983\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 28us/sample - loss: -2.3871 - val_loss: -2.3974\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 33us/sample - loss: -2.3870 - val_loss: -2.3972\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: -2.3886 - val_loss: -2.3978\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 27us/sample - loss: -2.3864 - val_loss: -2.3981\n",
      "Train on 19358 samples, validate on 4840 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19358/19358 [==============================] - 0s 23us/sample - loss: -2.3868 - val_loss: -2.3984\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 24us/sample - loss: -2.3878 - val_loss: -2.4067\n",
      "Trained model on 57 epochs for time 30.410444974899292 secs (1.874355999954873 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error: 9.138872917508202'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_poisson_twol_m(unscaled), batch_size=unscaled.X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 211us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 175us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 180us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 174us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 175us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 191us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      " 1280/19358 [>.............................] - ETA: 3s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0d3e6a382e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_poisson_expdec_threel_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}\"\u001b[0m\u001b[0;31m# , poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36mtrain_model_for_seconds\u001b[0;34m(dataset, model, seconds, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         cur = model.fit(dataset.X_train, dataset.y_train, batch_size=batch_size, epochs=1, \n\u001b[0;32m---> 15\u001b[0;31m                         validation_data=(dataset.X_dev, dataset.y_dev))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_model(unscaled, get_poisson_expdec_threel_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 164us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 151us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 157us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 4s 204us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 158us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 172us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 164us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 164us/sample - loss: nan - val_loss: nan\n",
      "Trained model on 10 epochs for time 32.4913432598114 secs (0.3077742868319334 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7cebb97d74e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_poisson_expdec_threel_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5547b6890dcd>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}, poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_tweedie_deviance\u001b[0;34m(y_true, y_pred, sample_weight, power)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# Poisson distribution, y_true >= 0, y_pred > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             raise ValueError(message + \"non-negative y_true and strictly \"\n\u001b[0m\u001b[1;32m    737\u001b[0m                              \"positive y_pred.\")\n\u001b[1;32m    738\u001b[0m         \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxlogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_poisson_expdec_threel_m(unscaled), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled on poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 1s 35us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 0s 25us/sample\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d83154c34616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompdiags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_poisson_expdec_threel_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompdiags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompdiags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}\"\u001b[0m\u001b[0;31m# , poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ff4dbddebb5a>\u001b[0m in \u001b[0;36mtrain_model_for_seconds\u001b[0;34m(dataset, model, seconds, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         cur = model.fit(dataset.X_train, dataset.y_train, batch_size=batch_size, epochs=1, \n\u001b[0;32m---> 15\u001b[0;31m                         validation_data=(dataset.X_dev, dataset.y_dev))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_model(compdiags, get_poisson_expdec_threel_m(compdiags), batch_size=compdiags.X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My custom poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 178us/sample - loss: -3.3802 - val_loss: -3.5021\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: -3.5032 - val_loss: -3.4992\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 160us/sample - loss: -3.5346 - val_loss: -3.4840\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 164us/sample - loss: -3.5540 - val_loss: -3.3531\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 133us/sample - loss: -3.5642 - val_loss: -3.4602\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 125us/sample - loss: -3.5978 - val_loss: -3.4660\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 2s 122us/sample - loss: -3.6228 - val_loss: -3.3690\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 133us/sample - loss: -3.6479 - val_loss: -3.3859\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 151us/sample - loss: -3.6850 - val_loss: -3.3026\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 146us/sample - loss: -3.7028 - val_loss: -3.3220\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: -3.7277 - val_loss: -3.1920\n",
      "Trained model on 11 epochs for time 32.30435347557068 secs (0.3405113805579939 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-bca0e6a62bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_my_poisson_twol_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5547b6890dcd>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}, poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_tweedie_deviance\u001b[0;34m(y_true, y_pred, sample_weight, power)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# Poisson distribution, y_true >= 0, y_pred > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             raise ValueError(message + \"non-negative y_true and strictly \"\n\u001b[0m\u001b[1;32m    737\u001b[0m                              \"positive y_pred.\")\n\u001b[1;32m    738\u001b[0m         \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxlogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_my_poisson_twol_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 170us/sample - loss: -1.8034 - val_loss: -2.2944\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 162us/sample - loss: -2.2210 - val_loss: -2.2116\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 160us/sample - loss: -2.3464 - val_loss: -2.0505\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 174us/sample - loss: -2.3201 - val_loss: -2.2506\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 179us/sample - loss: -2.3805 - val_loss: -2.2826\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 151us/sample - loss: -2.5101 - val_loss: -2.1867\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 135us/sample - loss: -2.5078 - val_loss: -2.1068\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 166us/sample - loss: -2.5359 - val_loss: -2.0254\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: -2.5434 - val_loss: -1.8207\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 161us/sample - loss: -2.5730 - val_loss: -1.6550\n",
      "Trained model on 10 epochs for time 31.928866386413574 secs (0.31319621182214025 epochs in second)\n",
      "Loss history:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-7bb32d1d7f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_poisson_twol_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5547b6890dcd>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(dataset, model, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_model_for_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"mean squared error: {mean_squared_error(dataset.y_dev, predictions)}, poisson: {mean_tweedie_deviance(dataset.y_dev, predictions, power=1)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_tweedie_deviance\u001b[0;34m(y_true, y_pred, sample_weight, power)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# Poisson distribution, y_true >= 0, y_pred > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             raise ValueError(message + \"non-negative y_true and strictly \"\n\u001b[0m\u001b[1;32m    737\u001b[0m                              \"positive y_pred.\")\n\u001b[1;32m    738\u001b[0m         \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxlogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Tweedie deviance error with power=1 can only be used on non-negative y_true and strictly positive y_pred."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(unscaled, get_poisson_twol_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diffs_distr(df, model):\n",
    "    eval_model(df, model)\n",
    "    preds = model.predict(unscaled.X_dev)\n",
    "    difs = preds[:, 0] - unscaled.y_dev\n",
    "    \n",
    "    print()\n",
    "    plt.hist(difs, bins = 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 162us/sample - loss: 10.2009 - val_loss: 9.9078\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: 9.4306 - val_loss: 10.3397\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 154us/sample - loss: 8.7450 - val_loss: 10.1134\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: 8.6252 - val_loss: 9.5664\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: 8.5316 - val_loss: 11.5307\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: 8.6178 - val_loss: 9.5931\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 159us/sample - loss: 8.6980 - val_loss: 9.6582\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 163us/sample - loss: 8.4117 - val_loss: 10.8544\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 159us/sample - loss: 8.3738 - val_loss: 53.5992\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 159us/sample - loss: 8.3016 - val_loss: 11.0444\n",
      "Trained model on 10 epochs for time 31.126219987869263 secs (0.3212725478357884 epochs in second)\n",
      "Loss history:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_diffs_distr(unscaled, get_two_layer_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 164us/sample - loss: -1.2752 - val_loss: -2.4511\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: -2.3176 - val_loss: -2.2564\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 157us/sample - loss: -2.4457 - val_loss: -2.4659\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: -2.4851 - val_loss: -2.2565\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: -2.5145 - val_loss: -2.3267\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: -2.4966 - val_loss: -2.2755\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 151us/sample - loss: -2.5580 - val_loss: -2.2029\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 151us/sample - loss: -2.3789 - val_loss: -1.4381\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 152us/sample - loss: -2.4036 - val_loss: -1.6380\n",
      "Train on 19358 samples, validate on 4840 samples\n",
      "19358/19358 [==============================] - 3s 156us/sample - loss: -2.4416 - val_loss: -1.3466\n",
      "Trained model on 10 epochs for time 30.45710802078247 secs (0.328330581917905 epochs in second)\n",
      "Loss history:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_diffs_distr(unscaled, get_poisson_twol_m(unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
